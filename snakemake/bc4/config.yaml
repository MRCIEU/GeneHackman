cluster:
  export USER=$(whoami) &&
  export ACCOUNT_ID=$(sacctmgr show user withassoc format=account where user=$USER | grep smed) &&
  mkdir -p /user/work/$USER/slurm_logs &&
  sbatch
    --partition={resources.partition}
    --account=$ACCOUNT_ID
    --nodes={resources.nodes}
    --ntasks-per-node={resources.ntasks_per_node}
    --cpus-per-task={resources.ntasks_per_node}
    --time={resources.time}
    --cpus-per-task={threads}
    --mem={resources.mem}
    --job-name={rule}-{wildcards}
    --output=/user/work/$USER/slurm_logs/{rule}{wildcards}_%j.out
default-resources:
  - time='01:00:00'
  - partition=mrcieu
  - mem='1G'
  - ntasks_per_node=1
  - nodes=1
restart-times: 0
max-jobs-per-second: 10
max-status-checks-per-second: 1
local-cores: 1
latency-wait: 10
jobs: 10
keep-going: True
rerun-incomplete: True
use-singularity: True
singularity-args: '-B $(pwd)/R:/home/R -B $(pwd)/scripts:/home/scripts -B /user/work/$(whoami) -B /user/home/$(whoami) -B /mnt/storage/private/mrcieu/data/ --env-file .env --pwd /home/scripts/'
printshellcmds: True
scheduler: greedy
